{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Email Spam ( Medium Part 2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woPNcQEXJgz3",
    "colab_type": "text"
   },
   "source": [
    "# URL that is helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVewEExEJgz6",
    "colab_type": "text"
   },
   "source": [
    "1. [Preprocessing Technique](https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908)\n",
    "2. [Convert text into features](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)\n",
    "3. [Using word embedding](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpa_bP5cKKxX",
    "colab_type": "text"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fyn-bhBDKO4P",
    "colab_type": "text",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Download the dataset\n",
    "\n",
    "# !mkdir -p data/spam_data\n",
    "# !ls data\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\n",
    "#\n",
    "# !tar xvjf 20030228_easy_ham_2.tar.bz2\n",
    "# !tar xvjf 20030228_easy_ham.tar.bz2\n",
    "# !tar xvjf 20030228_hard_ham.tar.bz2\n",
    "# !tar xvjf 20030228_spam.tar.bz2\n",
    "# !tar xvjf 20050311_spam_2.tar.bz2\n",
    "#\n",
    "# !mv easy_ham data/spam_data\n",
    "# !mv easy_ham_2 data/spam_data\n",
    "# !mv hard_ham data/spam_data\n",
    "# !mv spam data/spam_data\n",
    "# !mv spam_2 data/spam_data"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PKJ-Hz6VJg0T",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Download the glove embedding\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip\n",
    "# !python -m gensim.scripts.glove2word2vec -i glove.6B.300d.txt -o glove.6B.300d.word2vec.txt"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dGCmvDs7Jg0W",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import email\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\59384\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\59384\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7c7DqHJvJg0y",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Define the file path\n",
    "\n",
    "path = 'data/spam_data/'\n",
    "easy_ham_paths = glob.glob(path+'easy_ham/*')\n",
    "easy_ham_2_paths = glob.glob(path+'easy_ham_2/*')\n",
    "hard_ham_paths = glob.glob(path+'hard_ham/*')\n",
    "spam_paths = glob.glob(path+'spam/*')\n",
    "spam_2_paths = glob.glob(path+'spam_2/*')"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lkVocQJTJg0-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def get_email_content(email_path):\n",
    "    file = open(email_path,encoding='latin1')\n",
    "    try:\n",
    "        msg = email.message_from_file(file)\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                return part.get_payload() # prints the raw text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def get_email_content_bulk(email_paths):\n",
    "    email_contents = [get_email_content(o) for o in email_paths]\n",
    "    return email_contents"
   ],
   "execution_count": 70,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aCWVWIByJg1B",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Split data into train/test\n",
    "non_spam_path = easy_ham_paths + easy_ham_2_paths + hard_ham_paths\n",
    "spam_path = spam_paths+spam_2_paths"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
      "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "\n",
      "  | I can't reproduce this error.\n",
      "\n",
      "For me it is very repeatable... (like every time, without fail).\n",
      "\n",
      "This is the debug log of the pick happening ...\n",
      "\n",
      "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
      "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
      "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
      "18:19:04 Marking 1 hits\n",
      "18:19:04 tkerror: syntax error in expression \"int ...\n",
      "\n",
      "Note, if I run the pick command by hand ...\n",
      "\n",
      "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
      "1 hit\n",
      "\n",
      "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
      "using is ...\n",
      "\n",
      "delta$ pick -version\n",
      "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
      "\n",
      "And the relevant part of my .mh_profile ...\n",
      "\n",
      "delta$ mhparam pick\n",
      "-seq sel -list\n",
      "\n",
      "\n",
      "Since the pick command works, the sequence (actually, both of them, the\n",
      "one that's explicit on the command line, from the search popup, and the\n",
      "one that comes from .mh_profile) do get created.\n",
      "\n",
      "kre\n",
      "\n",
      "ps: this is still using the version of the code form a day ago, I haven't\n",
      "been able to reach the cvs repository today (local routing issue I think).\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "Exmh-workers mailing list\n",
      "Exmh-workers@redhat.com\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading the email, just use one email for testing purposes\n",
    "sample = non_spam_path[0]\n",
    "# print(sample)\n",
    "file = open(sample)\n",
    "try:\n",
    "    msg = email.message_from_file(file)\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            body = part.get_payload()\n",
    "            print(body)\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 5216 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(non_spam_path[0], 'rb').read().decode(encoding='utf-8')\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'utf-8' codec can't decode byte 0xb4 in position 93: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xe8 in position 565: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xe9 in position 2073: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xe9 in position 1965: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xe9 in position 1456: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xe9 in position 1457: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xab in position 2130: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xb7 in position 1139: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xe9 in position 1652: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xab in position 1056: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xab in position 367: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xab in position 454: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xe9 in position 1452: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xa3 in position 529: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x80 in position 211: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xd8 in position 3221: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xa3 in position 1334: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x96 in position 1873: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xe1 in position 22: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0x99 in position 562: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xee in position 1397: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0x97 in position 353: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x92 in position 827: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x97 in position 861: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x85 in position 1991: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xae in position 1056: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x97 in position 441: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xb0 in position 81: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xe9 in position 1746: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xa7 in position 117: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x82 in position 59: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x97 in position 816: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x93 in position 7: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x97 in position 861: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xee in position 2231: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0x82 in position 59: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x82 in position 59: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x82 in position 59: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x99 in position 586: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x92 in position 827: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xbc in position 580: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xff in position 2624: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xe9 in position 224: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xe7 in position 741: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xc6 in position 27: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xc6 in position 27: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xcb in position 21: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xe7 in position 741: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xe7 in position 741: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xc5 in position 105: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xb7 in position 130: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xf4 in position 1169: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xf4 in position 1169: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xa6 in position 229: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa6 in position 229: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x99 in position 449: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa1 in position 312: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa0 in position 119: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xb1 in position 2092: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xfe in position 102: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xfe in position 102: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa0 in position 42: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xbb in position 0: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x92 in position 177: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xe8 in position 826: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xe8 in position 826: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xdc in position 466: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0x95 in position 271: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xae in position 363: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xdd in position 11: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xdd in position 0: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0x96 in position 25: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa0 in position 122: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa0 in position 122: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xfc in position 8: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xfc in position 8: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x99 in position 130: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xe2 in position 416: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xb7 in position 620: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xb7 in position 531: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa0 in position 10765: invalid start byte\n",
      "'utf-8' codec can't decode byte 0x92 in position 700: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa3 in position 260: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa3 in position 260: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa1 in position 295: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xb7 in position 15: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xb7 in position 15: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xb7 in position 15: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xb7 in position 15: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xae in position 1537: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xb1 in position 7: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xf1 in position 226: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0x94 in position 927: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xf1 in position 278: invalid continuation byte\n",
      "'utf-8' codec can't decode byte 0xae in position 1075: invalid start byte\n",
      "'utf-8' codec can't decode byte 0xa1 in position 1075: invalid start byte\n",
      "Total number of message: 5005\n",
      "number of Non-Spam sentence: 4035== number of non_spam_labels: 4035\n",
      "number of Spam sentence: 970== number of Spam_labels: 970\n"
     ]
    }
   ],
   "source": [
    "non_spam_sentences = []\n",
    "spam_sentences = []\n",
    "non_spam_labels = []\n",
    "spam_labels = []\n",
    "count = 0\n",
    "\n",
    "# Extract the body content for non-spam message\n",
    "for path in non_spam_path:\n",
    "    file = open(path, encoding=\"utf8\", errors='ignore')\n",
    "    try:\n",
    "        msg = email.message_from_file(file)\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                body = part.get_payload(decode=True).decode()\n",
    "                non_spam_sentences.append(body)\n",
    "                non_spam_labels.append(1)\n",
    "                count+=1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Extract the body content for spam message\n",
    "for path in spam_path:\n",
    "    file = open(path, encoding=\"utf8\", errors='ignore')\n",
    "    try:\n",
    "        msg = email.message_from_file(file)\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                body = part.get_payload(decode=True).decode()\n",
    "                spam_sentences.append(body)\n",
    "                spam_labels.append(0)\n",
    "                count+=1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Expect: the number of spam_sentences == number of spam_labels\n",
    "print(f'Total number of message: {count}')\n",
    "print(f\"number of Non-Spam sentence: {len(non_spam_sentences)}== number of non_spam_labels: {len(non_spam_labels)}\")\n",
    "print(f\"number of Spam sentence: {len(spam_sentences)}== number of Spam_labels: {len(spam_labels)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Define the variables\n",
    "vocab_size = 1000 # Size of word corpus\n",
    "embedding_dim = 16 # dimension of output vector\n",
    "max_length = 120 # Maximum length of token\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<ooV>'\n",
    "training_portion = .8\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected the number of len(non_spam_labels) equals to the sum of len(train_labels)+len(valid_labels)\n",
      "Expected 3228+807==4035\n",
      "\n",
      "\n",
      "number of x_train: 4004\n",
      "number of y_train: 4004\n",
      "number of x_test: 1001\n",
      "number of y_test: 1001\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(non_spam_labels) * training_portion)\n",
    "# Split the trainign and testing dataset\n",
    "x_train = non_spam_sentences[:train_size]\n",
    "y_train = non_spam_labels[:train_size]\n",
    "x_test = non_spam_sentences[train_size:]\n",
    "y_test = non_spam_labels[train_size:]\n",
    "print(\"Expected the number of len(non_spam_labels) equals to the sum of len(train_labels)+len(valid_labels)\")\n",
    "print(f'Expected {len(y_train)}+{len(y_test)}=={len(non_spam_labels)}')\n",
    "\n",
    "train_size = int(len(spam_labels) * training_portion)\n",
    "x_train += spam_sentences[:train_size]\n",
    "y_train += spam_labels[:train_size]\n",
    "x_test += spam_sentences[train_size:]\n",
    "y_test += spam_labels[train_size:]\n",
    "\n",
    "print('\\n')\n",
    "print(f'number of x_train: {len(x_train)}')\n",
    "print(f'number of y_train: {len(y_train)}')\n",
    "print(f'number of x_test: {len(x_test)}')\n",
    "print(f'number of y_test: {len(y_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-spam file: 4153\n",
      "Number of spam file: 1898\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of non-spam file: {len(non_spam_path)}')\n",
    "print(f'Number of spam file: {len(spam_path)}')\n",
    "\n",
    "# Expected:\n",
    "# Number of non-spam file: 4153\n",
    "# Number of spam file: 1898"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Spam sample: \n",
      "    Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
      "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "\n",
      "  | I can't reproduce this error.\n",
      "\n",
      "For me it is very repeatable... (like every time, without fail).\n",
      "\n",
      "This is the debug log of the pick happening ...\n",
      "\n",
      "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
      "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
      "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
      "18:19:04 Marking 1 hits\n",
      "18:19:04 tkerror: syntax error in expression \"int ...\n",
      "\n",
      "Note, if I run the pick command by hand ...\n",
      "\n",
      "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
      "1 hit\n",
      "\n",
      "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
      "using is ...\n",
      "\n",
      "delta$ pick -version\n",
      "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
      "\n",
      "And the relevant part of my .mh_profile ...\n",
      "\n",
      "delta$ mhparam pick\n",
      "-seq sel -list\n",
      "\n",
      "\n",
      "Since the pick command works, the sequence (actually, both of them, the\n",
      "one that's explicit on the command line, from the search popup, and the\n",
      "one that comes from .mh_profile) do get created.\n",
      "\n",
      "kre\n",
      "\n",
      "ps: this is still using the version of the code form a day ago, I haven't\n",
      "been able to reach the cvs repository today (local routing issue I think).\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "Exmh-workers mailing list\n",
      "Exmh-workers@redhat.com\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the message content\n",
    "print(f\"Non-Spam sample: \\n{non_spam_sentences[0]}\")\n",
    "# print(f\"Spam sample: {spam_sentences[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gIfT1_6dJg2S",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "outputId": "64ed5b0a-0ffc-401c-bc0f-d5eb03a54759"
   },
   "source": [
    "## Process word by word\n",
    "# def remove_null(datas,labels):\n",
    "#     not_null_idx = [i for i,o in enumerate(datas) if o is not None]\n",
    "#     return np.array(datas)[not_null_idx],np.array(labels)[not_null_idx]\n",
    "# x_train,y_train = remove_null(x_train,y_train)\n",
    "# x_test,y_test = remove_null(x_test,y_test)\n",
    "\n",
    "print('\\n')\n",
    "print(f'number of x_train: {len(x_train)}')\n",
    "print(f'number of y_train: {len(y_train)}')\n",
    "print(f'number of x_test: {len(x_test)}')\n",
    "print(f'number of y_test: {len(y_test)}')"
   ],
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "number of x_train: 4004\n",
      "number of y_train: 4004\n",
      "number of x_test: 1001\n",
      "number of y_test: 1001\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "\"I can't reproduce this error For me it is very repeatable like every time without fail This is the debug log of the pick happening Pick It exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequence mercury exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequence mercury Ftoc PickMsgs hit Marking hits tkerror syntax error in expression int Note if I run the pick command by hand delta pick inbox list lbrace lbrace subject ftp rbrace rbrace sequence mercury hit That's where the hit comes from obviously The version of nmh I'm using is delta pick version pick nmh compiled on fuchsia cs mu OZ AU at Sun Mar ICT And the relevant part of my mh profile delta mhparam pick seq sel list Since the pick command works the sequence actually both of them the one that's explicit on the command line from the search popup and the one that comes from mh profile do get created kre ps this is still using the version of the code form a day ago I haven't been able to reach the cvs repository today local routing issue I think Exmh workers mailing list\""
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------Testing the cleaning operation on single sample--------------------\n",
    "msg = x_train[0]\n",
    "# The upper and lower case should have no impact to the result(neg/pos), so we should convert all chars to lower case, to reduce the overhead of traning process.\n",
    "msg.lower()\n",
    "# Using regex to remove any not-chars or \\', hyperlinks, emails addr, and same header information\n",
    "pattern = '[^a-zA-Z\\']|\\_+|http\\S+|\\S*@\\S*\\s?|Date:.*|From:.*|Message.*'\n",
    "replace = \" \"\n",
    "msg = re.sub(pattern, replace, msg).strip()\n",
    "# Calling re.sub again to remove those redundant whitespace\n",
    "msg = re.sub('[\\r\\n\\t ]+', replace, msg).strip()\n",
    "msg\n",
    "# msg = [i for i in msg if i not in stopwords]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "#----------------------Applying the cleaning operation on all trainign data --------------------\n",
    "def msg_cleaning(msg):\n",
    "    # The upper and lower case should have no impact to the result(neg/pos), so we should convert all chars to lower case, to reduce the overhead of traning process.\n",
    "    msg.lower()\n",
    "    # Using regex to remove any not-chars or \\', hyperlinks, emails addr, and same header information\n",
    "    pattern = '[^a-zA-Z\\']|\\_+|http\\S+|\\S*@\\S*\\s?|Date:.*|From:.*|Message.*'\n",
    "    replace = \" \"\n",
    "    msg = re.sub(pattern, replace, msg).strip()\n",
    "    # Calling re.sub again to remove those redundant whitespace\n",
    "    msg = re.sub('[\\r\\n\\t ]+', replace, msg).strip()\n",
    "    # Remove the any word in stopwords from msg (Those words has no semantic meaning to our model, so we should remove them to reduce the training overhead)\n",
    "    msg = msg.split(' ')\n",
    "    msg = [word for word in msg if word not in stopwords]\n",
    "    msg = \" \".join(msg)\n",
    "    return msg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "msg = x_train[0]\n",
    "msg = msg_cleaning(msg)\n",
    "msg = msg.split(' ')\n",
    "msg = [word for word in msg if word not in stopwords]\n",
    "msg = \" \".join(msg)\n",
    "msg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Process the data in the word-by-word fasion: Spcifically, for each word, if it's in the set of stopwords, remove it.\n",
    "count = 0\n",
    "for idx, msg in enumerate(x_train):\n",
    "    x_train[idx] = msg_cleaning(msg)\n",
    "    count+=1\n",
    "    print(count)\n",
    "for idx, msg in enumerate(x_test):\n",
    "    x_test[idx] = msg_cleaning(msg)\n",
    "    count+=1\n",
    "    print(count)\n",
    "print('\\n')\n",
    "print(f'number of x_train: {len(x_train)}')\n",
    "print(f'number of y_train: {len(y_train)}')\n",
    "print(f'number of x_test: {len(x_test)}')\n",
    "print(f'number of y_test: {len(y_test)}')\n",
    "# Now the msg looks much more clean, let's begin the Tokenization process."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "msg = [lemmatizer.lemmatize(o) for o in msg]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect the message content\n",
    "print(f\"Non-Spam sample: \\n{non_spam_sentences[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gniRm08jJg2U",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "outputId": "f07b57f3-bc43-487c-ff42-03b94179163c"
   },
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SLaROjMvJg2X",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "outputId": "5e30244e-a01c-4531-cadb-a1c93f5a5337"
   },
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1BQ3-ohaJg2b",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "outputId": "59e4be06-cfcc-4bb4-811c-310e3d173538"
   },
   "source": [
    "x_train = [word_tokenize(o) for o in train_sentences]\n",
    "x_test = [word_tokenize(o) for o in valid_sentences]"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def word_stemmer(words):\n",
    "    return [stemmer.stem(o) for o in words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def word_lemmatizer(words):\n",
    "    return [lemmatizer.lemmatize(o) for o in words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_token_pipeline(words):\n",
    "    cleaning_utils = [remove_stop_words,word_stemmer,word_lemmatizer]\n",
    "    for o in cleaning_utils:\n",
    "        words = o(words)\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "x_train = [clean_token_pipeline(o) for o in x_train]\n",
    "x_test = [clean_token_pipeline(o) for o in x_test]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nOlphmFrJg2q",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Play around with Word2vector"
   ],
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zFgjFkSRJg2r",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format('glove.6B.300d.word2vec.txt',binary=False)"
   ],
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "crmVKUQNJg2t",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "w2v.most_similar('king')"
   ],
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "[('queen', 0.6336468458175659),\n ('prince', 0.619662344455719),\n ('monarch', 0.5899620652198792),\n ('kingdom', 0.5791266560554504),\n ('throne', 0.5606487989425659),\n ('ii', 0.5562329292297363),\n ('iii', 0.5503199100494385),\n ('crown', 0.5224862694740295),\n ('reign', 0.521735429763794),\n ('kings', 0.5066401362419128)]"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m9ZXFPTgJg2x",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "outputId": "033c153d-f70c-471a-ccf1-7f3e85b5bd3a"
   },
   "source": [
    "# King + Woman - Man = ?\n",
    "w2v.most_similar(['king','woman'],negative=['man'],topn=1)"
   ],
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "[('queen', 0.6713277101516724)]"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SZYY8gJmJg20",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "w2v.doesnt_match(\"england china vietnam laos\".split())"
   ],
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": "'england'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hon62hmIJg21",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 888
    },
    "outputId": "8952f167-dc5e-4afd-d589-8fdccd952daf"
   },
   "source": [
    "w2v.doesnt_match(\"fish shark cat whale\".split())"
   ],
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "'cat'"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bkSl2rJxJg23",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "outputId": "6d38a67a-7aba-4151-e128-06a02e234c01"
   },
   "source": [
    "# Extract Features from Words"
   ],
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOoYojWtJg3O",
    "colab_type": "text"
   },
   "source": [
    "## Use Glove Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zncorqyvJg3O",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ],
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In c:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In c:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In c:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In c:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In c:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In c:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In c:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In c:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p9W9PJZDJg3S",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "maxlen = 2000\n",
    "max_features = 50000"
   ],
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tfrdY0NbJg3U",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "EMBEDDING_FILE = 'glove.6B.300d.txt'\n",
    "tokenizer = Tokenizer(num_words=max_features)"
   ],
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0ulmgu2_Jg3V",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "tokenizer.fit_on_texts(x_train)"
   ],
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "15_Olth-Jg3W",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "x_train_features = np.array(tokenizer.texts_to_sequences(x_train))\n",
    "x_test_features = np.array(tokenizer.texts_to_sequences(x_test))\n",
    "\n",
    "x_train_features = pad_sequences(x_train_features,maxlen=maxlen)\n",
    "x_test_features = pad_sequences(x_test_features,maxlen=maxlen)"
   ],
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "STmnfjEsJg3X",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]"
   ],
   "execution_count": 54,
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 6148: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-54-2ed193a86bd2>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mget_coefs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0marr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;32mreturn\u001B[0m \u001B[0mword\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'float32'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0membeddings_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_coefs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mo\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\" \"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mo\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEMBEDDING_FILE\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mall_embs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membeddings_index\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0memb_mean\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0memb_std\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mall_embs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mall_embs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-54-2ed193a86bd2>\u001B[0m in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mget_coefs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0marr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;32mreturn\u001B[0m \u001B[0mword\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'float32'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0membeddings_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_coefs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mo\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\" \"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mo\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEMBEDDING_FILE\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mall_embs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membeddings_index\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0memb_mean\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0memb_std\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mall_embs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mall_embs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\59384\\anaconda3\\envs\\facecourse-py3\\lib\\encodings\\cp1252.py\u001B[0m in \u001B[0;36mdecode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mIncrementalDecoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcodecs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIncrementalDecoder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfinal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mcodecs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcharmap_decode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdecoding_table\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mStreamWriter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mCodec\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcodecs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mStreamWriter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'charmap' codec can't decode byte 0x9d in position 6148: character maps to <undefined>"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TxXSia3XJg3Z",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "outputId": "2f0b12ae-71cf-4fe7-a598-13e6f230e77c"
   },
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wLR9OTUGJg3g",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "outputId": "b4cfaf90-73b4-4c25-d389-08e1c50e9266"
   },
   "source": [
    "model.layers[1].trainable = False"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o6ddZ8E1Jg3k",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "outputId": "4a651eb0-71bd-4dcb-a73b-bd7378170c01"
   },
   "source": [
    "history = model.fit(x_train_features,y_train, batch_size=512, epochs=20, \n",
    "          validation_data=(x_test_features, y_test))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Neural Network w Embedding From Scratch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gkJQ-yURJg3n",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Neural Network w Embedding From Scratch"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gB21196PJg3o",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "fa43c0dc-6bce-482c-cb44-b6d8ff22009b"
   },
   "source": [
    "## some config values \n",
    "embed_size = 100 # how big is each word vector\n",
    "max_feature = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "max_len = 2000 # max number of words in a question to use"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kberNYWIJg3r",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "tokenizer = Tokenizer(num_words=max_feature)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kshhXwFAJg3s",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "1f43f247-d0d7-4b46-da41-740fefb75290"
   },
   "source": [
    "tokenizer.fit_on_texts(x_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x8pacWZkJg3v",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "5789484b-9ebf-4673-f7f6-b3e732f6efc9"
   },
   "source": [
    "x_train_features = np.array(tokenizer.texts_to_sequences(x_train))\n",
    "x_test_features = np.array(tokenizer.texts_to_sequences(x_test))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09TivZGrJg3y",
    "colab_type": "text"
   },
   "source": [
    "x_train_features = pad_sequences(x_train_features,maxlen=max_len)\n",
    "x_test_features = pad_sequences(x_test_features,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KJyDyVJHJg3y",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "embed_size = 100\n",
    "\n",
    "inp = Input(shape=(max_len,))\n",
    "x = Embedding(max_feature, embed_size)(inp)\n",
    "x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78278baOJg3z",
    "colab_type": "text"
   },
   "source": [
    "history = model.fit(x_train_features, y_train, batch_size=512, epochs=20, validation_data=(x_test_features, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xSafPEb4Jg3z",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "outputId": "8218b62e-31e2-4ddc-9b71-f87066deed3c"
   },
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TRASX4txVNJS",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Neural Network"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}